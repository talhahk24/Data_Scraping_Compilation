{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talhahk24/Data_Scraping_Compilation/blob/main/(June_2024)_Food_Data_Scraping_(Center_for_Food_and_Safety).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Data"
      ],
      "metadata": {
        "id": "oDjd46nwwSig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n"
      ],
      "metadata": {
        "id": "C98a7PrHxaCp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Gcrjjjx2Rzt9"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SETUP CELL - All Installation Commands and Selenium Initialization\n",
        "# =============================================================================\n",
        "\n",
        "!pip install selenium\n",
        "!pip install pandas\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install sacremoses\n",
        "!pip install --upgrade google-cloud-translate\n",
        "\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0, '/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import Select\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException, ElementClickInterceptedException\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import transformers as pipeline\n",
        "from google.colab import files, auth\n",
        "from google.cloud import translate_v3 as translate\n",
        "\n",
        "chrome_options = Options()\n",
        "\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "print(\" Setup complete! All packages installed and WebDriver initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Functions"
      ],
      "metadata": {
        "id": "M9xaHqNtxdBh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG1bkqSqX8Mc"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SCRAPING FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "class FoodDataScraper:\n",
        "    \"\"\"\n",
        "    A modular class for scraping food data from the Center for Food and Safety website.\n",
        "    Handles navigation, data extraction, and data processing for multiple languages.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, driver, wait_timeout=20):\n",
        "        self.driver = driver\n",
        "        self.wait = WebDriverWait(driver, wait_timeout)\n",
        "        self.all_data = pd.DataFrame()\n",
        "\n",
        "        self.category_ids = [\"17\", \"13\", \"29\", \"01\", \"20\", \"09\", \"15\", \"12\", \"11\", \"04\",\n",
        "                           \"08\", \"19\", \"02\", \"06\", \"10\", \"16\", \"05\", \"07\", \"40\", \"22\",\n",
        "                           \"26\", \"21\", \"18\", \"03\"]\n",
        "        self.grp_ids = ['grp17', 'grp13', 'grp29', 'grp01', 'grp20', 'grp09', 'grp15',\n",
        "                       'grp12', 'grp11', 'grp04', 'grp08', 'grp19', 'grp02', 'grp06',\n",
        "                       'grp10', 'grp16', 'grp05', 'grp07', 'grp40', 'grp22', 'grp26',\n",
        "                       'grp21', 'grp18', 'grp03']\n",
        "\n",
        "    def navigate_to_category(self, category_id, grp_id):\n",
        "        \"\"\"Navigate to a specific food category and its group page.\"\"\"\n",
        "        try:\n",
        "            category_link = self.wait.until(\n",
        "                EC.element_to_be_clickable((By.XPATH, f\"//td[@id='{category_id}']//a\"))\n",
        "            )\n",
        "            category_name = category_link.text.strip()\n",
        "            ActionChains(self.driver).move_to_element(category_link).click().perform()\n",
        "\n",
        "            self.wait.until(EC.presence_of_element_located((By.ID, grp_id)))\n",
        "\n",
        "            return category_name\n",
        "        except Exception as e:\n",
        "            print(f\"Error navigating to category {category_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def open_data_tab(self, grp_id):\n",
        "        \"\"\"Open the data tab for a specific group.\"\"\"\n",
        "        try:\n",
        "            grp_link = self.wait.until(\n",
        "                EC.element_to_be_clickable((By.XPATH, f\"//td[@id='{grp_id}']/a\"))\n",
        "            )\n",
        "\n",
        "            self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", grp_link)\n",
        "\n",
        "            original_window = self.driver.current_window_handle\n",
        "            windows_before = self.driver.window_handles\n",
        "\n",
        "            ActionChains(self.driver).move_to_element(grp_link).click().perform()\n",
        "\n",
        "            self.wait.until(EC.new_window_is_opened(windows_before))\n",
        "\n",
        "            windows_after = self.driver.window_handles\n",
        "            new_window = [window for window in windows_after if window not in windows_before][0]\n",
        "            self.driver.switch_to.window(new_window)\n",
        "\n",
        "            self.wait.until(EC.presence_of_element_located((By.ID, 'content')))\n",
        "\n",
        "            return original_window\n",
        "        except Exception as e:\n",
        "            print(f\"Error opening data tab for {grp_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def extract_category_name(self, language='en'):\n",
        "        \"\"\"Extract the category name from the page based on language.\"\"\"\n",
        "        try:\n",
        "            category_table = self.driver.find_element(By.CSS_SELECTOR, 'table.colorTable1')\n",
        "\n",
        "            patterns = {\n",
        "                'en': 'Food Group :',\n",
        "                'tc': '食物類別 :',\n",
        "                'sc': '食物类别 :'\n",
        "            }\n",
        "\n",
        "            pattern = patterns.get(language, patterns['en'])\n",
        "            food_group_label_td = category_table.find_element(\n",
        "                By.XPATH, f\".//td[contains(text(),'{pattern}')]\"\n",
        "            )\n",
        "            food_group_value_td = food_group_label_td.find_element(\n",
        "                By.XPATH, 'following-sibling::td'\n",
        "            )\n",
        "\n",
        "            return food_group_value_td.text.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting category name: {e}\")\n",
        "            return \"Unknown Category\"\n",
        "\n",
        "    def extract_table_data(self):\n",
        "        \"\"\"Extract data from the colorTable2 table.\"\"\"\n",
        "        try:\n",
        "            data_table = self.driver.find_element(By.CSS_SELECTOR, 'table.colorTable2')\n",
        "            rows = data_table.find_elements(By.XPATH, \".//tbody/tr\")\n",
        "\n",
        "            headers = []\n",
        "            data_rows = []\n",
        "            header_collected = False\n",
        "\n",
        "            for row in rows:\n",
        "                th_elements = row.find_elements(By.XPATH, \".//th\")\n",
        "\n",
        "                if th_elements and not header_collected:\n",
        "                    headers = [th.text.strip() for th in th_elements]\n",
        "                    if 'Category' not in headers:\n",
        "                        headers.append('Category')\n",
        "                    if 'FoodID' not in headers:\n",
        "                        headers.append('FoodID')\n",
        "                    header_collected = True\n",
        "                elif not th_elements:\n",
        "                    td_elements = row.find_elements(By.XPATH, \".//td\")\n",
        "                    data = [td.text.strip() for td in td_elements]\n",
        "\n",
        "                    if data:\n",
        "                        food_id = self._extract_food_id(row)\n",
        "\n",
        "                        data_dict = dict(zip(headers[:-2], data))\n",
        "                        data_dict['Category'] = None\n",
        "                        data_dict['FoodID'] = food_id\n",
        "                        data_rows.append(data_dict)\n",
        "\n",
        "            return pd.DataFrame(data_rows)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting table data: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def _extract_food_id(self, row):\n",
        "        \"\"\"Extract food ID from a table row.\"\"\"\n",
        "        try:\n",
        "            a_element = row.find_element(By.XPATH, \".//a\")\n",
        "            href = a_element.get_attribute('href')\n",
        "            match = re.search(r\"javascript:tosubmit\\('\\d+','-1','([^']+)'\\)\", href)\n",
        "            return match.group(1) if match else None\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def close_tab_and_return(self, original_window):\n",
        "        \"\"\"Close current tab and return to original window.\"\"\"\n",
        "        try:\n",
        "            self.driver.close()\n",
        "            self.driver.switch_to.window(original_window)\n",
        "            self.driver.back()\n",
        "            self.wait.until(EC.presence_of_element_located((By.ID, '17')))\n",
        "        except Exception as e:\n",
        "            print(f\"Error closing tab: {e}\")\n",
        "\n",
        "    def merge_dataframes(self, new_df, category_name):\n",
        "        \"\"\"Merge new DataFrame with existing data.\"\"\"\n",
        "        try:\n",
        "            new_df['Category'] = category_name\n",
        "\n",
        "            for col in new_df.columns:\n",
        "                if col not in self.all_data.columns:\n",
        "                    self.all_data[col] = None\n",
        "\n",
        "            for col in self.all_data.columns:\n",
        "                if col not in new_df.columns:\n",
        "                    new_df[col] = None\n",
        "\n",
        "            self.all_data = pd.concat([self.all_data, new_df], ignore_index=True, sort=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging dataframes: {e}\")\n",
        "\n",
        "    def scrape_single_category(self, category_id, grp_id, language='en'):\n",
        "        \"\"\"Scrape data for a single category.\"\"\"\n",
        "        try:\n",
        "            print(f\"Processing category {category_id}...\")\n",
        "\n",
        "            category_name = self.navigate_to_category(category_id, grp_id)\n",
        "            if not category_name:\n",
        "                return False\n",
        "\n",
        "            original_window = self.open_data_tab(grp_id)\n",
        "            if not original_window:\n",
        "                return False\n",
        "\n",
        "            self.driver.save_screenshot(f'Dataset_{category_name}.png')\n",
        "\n",
        "            page_category_name = self.extract_category_name(language)\n",
        "\n",
        "            category_df = self.extract_table_data()\n",
        "\n",
        "            if not category_df.empty:\n",
        "                self.merge_dataframes(category_df, page_category_name)\n",
        "                print(f\"Scraping Complete: {page_category_name}\")\n",
        "            else:\n",
        "                print(f\"No data found for {page_category_name}\")\n",
        "\n",
        "            self.close_tab_and_return(original_window)\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping category {category_id}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def scrape_all_categories(self, main_url, language='en', filename=None):\n",
        "        \"\"\"Scrape all categories for a given language.\"\"\"\n",
        "        try:\n",
        "            print(f\"Starting scraping for {language} version...\")\n",
        "\n",
        "            self.driver.get(main_url)\n",
        "            self.driver.save_screenshot('Main.png')\n",
        "\n",
        "            self.all_data = pd.DataFrame()\n",
        "\n",
        "            for i, category_id in enumerate(self.category_ids):\n",
        "                grp_id = self.grp_ids[i]\n",
        "                success = self.scrape_single_category(category_id, grp_id, language)\n",
        "\n",
        "                if not success:\n",
        "                    print(f\"Failed to scrape category {category_id}\")\n",
        "\n",
        "            if filename:\n",
        "                self.all_data.to_csv(filename, index=False)\n",
        "                print(f\"Data saved to {filename}\")\n",
        "\n",
        "            return self.all_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in scrape_all_categories: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "# Initialize the scraper\n",
        "scraper = FoodDataScraper(driver)\n",
        "print(\"Modular scraper initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataScraping Mains"
      ],
      "metadata": {
        "id": "musahQpJxkQS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r0mlOBf0q6D"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENGLISH VERSION SCRAPING\n",
        "# =============================================================================\n",
        "\n",
        "english_data = scraper.scrape_all_categories(\n",
        "    main_url='https://www.cfs.gov.hk/english/nutrient/search1.php',\n",
        "    language='en',\n",
        "    filename='Eng_With_ID_All_Categories_Data.csv'\n",
        ")\n",
        "\n",
        "print(f\"English scraping complete! Total records: {len(english_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3KrXja5Qxgv"
      },
      "outputs": [],
      "source": [
        "# After all categories are processed, save the DataFrame to CSV\n",
        "filename = 'Eng_With_ID_All_Categories_Data.csv'\n",
        "all_data.to_csv(filename, index=False)\n",
        "print(f\"Data saved to {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD31njSVcS8p"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRADITIONAL CHINESE VERSION SCRAPING\n",
        "# =============================================================================\n",
        "\n",
        "tc_data = scraper.scrape_all_categories(\n",
        "    main_url='https://www.cfs.gov.hk/tc_chi/nutrient/search1.php',\n",
        "    language='tc',\n",
        "    filename='TC_With_ID_All_Categories_Data.csv'\n",
        ")\n",
        "\n",
        "print(f\"Traditional Chinese scraping complete! Total records: {len(tc_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT809fuocv28"
      },
      "outputs": [],
      "source": [
        "# After all categories are processed, save the DataFrame to CSV\n",
        "filename = 'TC_With_ID_All_Categories_Data.csv'\n",
        "all_data.to_csv(filename, index=False)\n",
        "print(f\"Data saved to {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsMQDwYcczyA"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SIMPLIFIED CHINESE VERSION SCRAPING\n",
        "# =============================================================================\n",
        "\n",
        "sc_data = scraper.scrape_all_categories(\n",
        "    main_url='https://www.cfs.gov.hk/sc_chi/nutrient/search1.php',\n",
        "    language='sc',\n",
        "    filename='SC_With_ID_All_Categories_Data.csv'\n",
        ")\n",
        "\n",
        "print(f\"Simplified Chinese scraping complete! Total records: {len(sc_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI0CV3c6dJhr"
      },
      "outputs": [],
      "source": [
        "# After all categories are processed, save the DataFrame to CSV\n",
        "filename = 'SC_With_ID_All_Categories_Data.csv'\n",
        "all_data.to_csv(filename, index=False)\n",
        "print(f\"Data saved to {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df23DkIvBSbp"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DOWNLOAD FILES\n",
        "# =============================================================================\n",
        "\n",
        "files.download('Eng_With_ID_All_Categories_Data.csv')\n",
        "files.download('TC_With_ID_All_Categories_Data.csv')\n",
        "files.download('SC_With_ID_All_Categories_Data.csv')\n",
        "\n",
        "print(\"All files downloaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb2-5FZ8Yu_-"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\n📊 Sample Data Preview:\")\n",
        "english_data.head(10) if 'english_data' in locals() else print(\"Run the scraping cells first!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX_y-JeLDGkp"
      },
      "source": [
        "# Translating Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33iy_9ixQIb7"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xZOhKPjjljlB"
      },
      "outputs": [],
      "source": [
        "all_data = pd.read_csv('All_Categories_Data.csv')\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-zh-en\")\n",
        "\n",
        "def translate_text(text):\n",
        "    if pd.isnull(text) or text.strip() == '':\n",
        "        return ''\n",
        "    result = translator(text, max_length=512)\n",
        "    print(f\"{text} = {result}\")\n",
        "    return result[0]['translation_text']\n",
        "\n",
        "all_data['Food Name (English)'] = all_data['食物名称'].apply(translate_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sEA9Zt6fxQc2"
      },
      "outputs": [],
      "source": [
        "all_data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH-cDebdaM9S"
      },
      "outputs": [],
      "source": [
        "service_account_key_path = '/content/credentials.json'\n",
        "\n",
        "client = translate.TranslationServiceClient.from_service_account_file(service_account_key_path)\n",
        "\n",
        "project_id = 'foodnametranslation-436816'\n",
        "\n",
        "location = 'global'\n",
        "\n",
        "parent = f'projects/{project_id}/locations/{location}'\n",
        "\n",
        "def translate_text(text):\n",
        "    if pd.isnull(text) or text.strip() == '':\n",
        "        return ''\n",
        "    response = client.translate_text(\n",
        "        parent=parent,\n",
        "        contents=[text],\n",
        "        mime_type='text/plain',\n",
        "        source_language_code='zh',\n",
        "        target_language_code='en'\n",
        "    )\n",
        "    for translation in response.translations:\n",
        "        print(f\"{text} = {translation.translated_text}\")\n",
        "        return translation.translated_text\n",
        "all_data['Food Name (English)'] = all_data['食物名称'].apply(translate_text)\n",
        "\n",
        "all_data.to_csv('All_Categories_Data_with_English_Names.csv', index=False)\n",
        "\n",
        "print(\"Translation complete. Data saved to 'All_Categories_Data_with_English_Names.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8wTTf09kykS"
      },
      "outputs": [],
      "source": [
        "def translate_text_nullable(text):\n",
        "    if pd.isnull(text) or text.strip() == '':\n",
        "        return text\n",
        "    else:\n",
        "        response = client.translate_text(\n",
        "            parent=parent,\n",
        "            contents=[text],\n",
        "            mime_type='text/plain',\n",
        "            source_language_code='zh',\n",
        "            target_language_code='en'\n",
        "        )\n",
        "        translation = response.translations[0]\n",
        "        print(f\"{text} = {translation.translated_text}\")\n",
        "        return translation.translated_text\n",
        "\n",
        "all_data['Alias (English)'] = all_data['別名'].apply(translate_text_nullable)\n",
        "\n",
        "all_data.to_csv('All_Categories_Data_with_Alias_English_Names.csv', index=False)\n",
        "\n",
        "print(\"Translation of '別名' column complete. Translated values are stored in 'Alias (English)' column.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiITpCdhlEPl"
      },
      "outputs": [],
      "source": [
        "all_data.to_csv('All_Categories_Data_with_Alias_English_Names.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4HfFpvhmSEZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}